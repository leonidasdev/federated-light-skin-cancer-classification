# =============================================================================
# Federated Learning Configuration for HAM10000 with Flower
# =============================================================================
# This configuration runs federated learning simulation using Flower framework.
# 
# Usage:
#   python scripts/train_federated_flower.py --config configs/experiments/federated_ham10000_flower.yaml
#
# For quick tests, you can override settings via command line:
#   python scripts/train_federated_flower.py --config configs/experiments/federated_ham10000_flower.yaml \
#       --num-clients 4 --num-rounds 5 --strategy fedavg

experiment:
  name: "federated_ham10000_flower"
  description: "Federated learning on HAM10000 with Flower - FedAvg baseline"

# =============================================================================
# Model Configuration
# =============================================================================
model:
  name: "lmsvit_small"  # Options: lmsvit_tiny, lmsvit_small, lmsvit_base
  num_classes: 7        # HAM10000 has 7 skin lesion classes
  img_size: 224
  
  # LMS-ViT architecture (inherited from model variant)
  # embed_dim: 384 (small), 192 (tiny), 768 (base)
  # depth: 12
  # num_heads: 6

# =============================================================================
# Data Configuration
# =============================================================================
data:
  dataset: "ham10000"   # Options: ham10000, isic2018, isic2019, isic2020
  data_dir: "./data/HAM10000"
  img_size: 224
  batch_size: 32
  num_workers: 0        # Use 0 for simulation to avoid multiprocessing issues

# =============================================================================
# Federated Learning Configuration
# =============================================================================
federated:
  # Client configuration
  num_clients: 4        # Number of simulated clients
  
  # Round configuration
  num_rounds: 10        # Number of federated rounds
  local_epochs: 2       # Local training epochs per round
  
  # Client selection
  fraction_fit: 1.0     # Fraction of clients for training (1.0 = all)
  fraction_evaluate: 1.0 # Fraction of clients for evaluation
  
  # Aggregation strategy
  strategy: "fedavg"    # Options: fedavg, fedprox, fedadam, fedadagrad
  
  # FedProx specific (only used if strategy: fedprox)
  fedprox_mu: 0.01      # Proximal term coefficient
  
  # Data partitioning
  partition:
    strategy: "iid"     # Options: iid, dirichlet, pathological
    alpha: 0.5          # Dirichlet concentration (lower = more non-IID)
                        # Typical values: 0.1 (very non-IID), 0.5, 1.0 (mild)
    classes_per_client: 2  # For pathological partitioning

# =============================================================================
# Training Configuration
# =============================================================================
training:
  learning_rate: 0.0001
  weight_decay: 0.01
  gradient_clip: 1.0
  
  # These are applied locally at each client
  optimizer: "adamw"
  
  # No scheduler for initial tests (can be added later)
  scheduler: null

# =============================================================================
# Hardware Configuration
# =============================================================================
hardware:
  device: "cuda"        # Options: cuda, cpu
  seed: 42              # Random seed for reproducibility
  
  # Resource allocation for simulation
  # Flower/Ray will distribute these across clients
  num_cpus_per_client: 1
  num_gpus_per_client: 0.25  # Share GPU among clients

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  log_dir: "./logs/federated"
  save_dir: "./checkpoints/federated"
  log_interval: 1       # Log every N rounds
  save_best: true       # Save best global model
  tensorboard: false    # TensorBoard logging (TODO)
  wandb: false          # W&B logging (TODO)


# =============================================================================
# Experiment Variations (for reference)
# =============================================================================
# 
# To run non-IID experiments, change:
#   federated.partition.strategy: "dirichlet"
#   federated.partition.alpha: 0.1  # Very heterogeneous
#
# To increase local computation:
#   federated.local_epochs: 5
#
# To test with more clients:
#   federated.num_clients: 10
#   federated.fraction_fit: 0.5  # Sample 50% of clients per round
#
# To use FedProx for heterogeneous settings:
#   federated.strategy: "fedprox"
#   federated.fedprox_mu: 0.01
#
# =============================================================================
