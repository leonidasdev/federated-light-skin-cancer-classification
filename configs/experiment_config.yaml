# Experiment Configuration
# Centralized vs Federated Learning Comparison

experiment:
  name: "DSCATNet-FL-SkinCancer"
  description: "Evaluating DSCATNet in Federated Learning for Dermoscopic Classification"
  seed: 42
  
# Hardware settings
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true  # FP16 training

# Data settings
data:
  root_dir: "./data"
  img_size: 224
  val_split: 0.2
  test_split: 0.1
  
  # Preprocessing
  normalization: "imagenet"  # Options: imagenet, dermoscopy
  augmentation_level: "medium"  # Options: light, medium, heavy
  
  # Classification mode
  # Options:
  #   - multiclass: 7 classes (HAM10000/ISIC2018 compatible)
  #   - multiclass_8: 8 classes (includes SCC from ISIC2019)
  #   - binary: 2 classes (benign vs malignant)
  classification_mode: "multiclass"
  
  # Number of classes (auto-set based on classification_mode)
  # multiclass: 7, multiclass_8: 8, binary: 2
  num_classes: 7
  
  # Unknown label handling
  filter_unknown: true  # Filter out UNK and unknown labels
  
  # Class imbalance handling
  use_weighted_sampling: false  # Use WeightedRandomSampler
  use_class_weights: true  # Use class weights in loss function

# Centralized Training Baseline
centralized:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.0001
  early_stopping_patience: 15
  
  # Data: pooled from all 4 datasets
  pooled_data: true
  
# Federated Training Experiments
federated_experiments:
  # Experiment 1: Natural Non-IID (each client = different dataset)
  - name: "natural_noniid"
    description: "Each client trains on its own dataset"
    num_rounds: 100
    local_epochs: 3
    batch_size: 32
    
  # Experiment 2: IID baseline
  - name: "iid_baseline"
    description: "Pooled data uniformly distributed"
    num_rounds: 100
    local_epochs: 3
    batch_size: 32
    
  # Experiment 3: Varying local epochs
  - name: "local_epochs_sweep"
    description: "Effect of local epochs on convergence"
    num_rounds: 100
    local_epochs: [1, 3, 5]
    batch_size: 32
    
  # Experiment 4: Varying batch size
  - name: "batch_size_sweep"
    description: "Effect of batch size"
    num_rounds: 100
    local_epochs: 3
    batch_size: [16, 32, 64]

# Evaluation Metrics
metrics:
  classification:
    - accuracy
    - precision
    - recall
    - f1_score
    - auc_roc
    - confusion_matrix
    
  federated:
    - convergence_rounds
    - communication_cost
    - client_drift
    - round_time
    
  per_class:
    - sensitivity
    - specificity
    - balanced_accuracy

# Logging and Checkpoints
logging:
  log_dir: "./logs"
  checkpoint_dir: "./checkpoints"
  tensorboard: true
  wandb:
    enabled: false
    project: "dscatnet-fl"
    entity: null

# Reproducibility
reproducibility:
  deterministic: true
  benchmark: false
  seed: 42
