{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980cd8f3",
   "metadata": {},
   "source": [
    "# Federated vs Centralized: Model Comparison\n",
    "\n",
    "**Comparing DSCATNet Performance: Centralized (Original Paper) vs Federated Learning**\n",
    "\n",
    "This notebook provides a head-to-head comparison between:\n",
    "\n",
    "1. **Centralized Training** (Baseline) - Original DSCATNet paper approach  \n",
    "   *Reference: [DSCATNet: Dual-Scale Cross-Attention Vision Transformer](https://doi.org/10.1371/journal.pone.0312598)*\n",
    "   \n",
    "2. **Federated Learning** - Our implementation with non-IID data\n",
    "\n",
    "---\n",
    "\n",
    "## Research Context\n",
    "\n",
    "The original DSCATNet paper (PLOS ONE, 2024) demonstrates state-of-the-art performance on dermoscopy classification using centralized training. This thesis investigates whether similar performance can be achieved in a **privacy-preserving federated learning** setting where data remains distributed across multiple hospitals.\n",
    "\n",
    "### Key Questions:\n",
    "- How much accuracy is lost when moving from centralized to federated learning?\n",
    "- Do different non-IID distributions affect convergence differently?\n",
    "- Which classes suffer most from federated training?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90d4b2",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add project root\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project imports\n",
    "from src.models.dscatnet import create_dscatnet\n",
    "from src.data.datasets import UNIFIED_CLASSES, CLASS_NAMES as DATASET_CLASS_NAMES\n",
    "from src.data.preprocessing import get_val_transforms\n",
    "from src.evaluation.metrics import ModelEvaluator, EvaluationResults, compare_results\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "CLASS_NAMES = list(DATASET_CLASS_NAMES)\n",
    "print(f\"Classes: {CLASS_NAMES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06046879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Set paths to your trained models\n",
    "# =============================================================================\n",
    "\n",
    "# Centralized model (baseline)\n",
    "CENTRALIZED_CHECKPOINT = project_root / \"outputs\" / \"centralized_YYYYMMDD_HHMMSS\" / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "# Federated model(s) - can compare multiple FL experiments\n",
    "FEDERATED_CHECKPOINTS = {\n",
    "    \"FL (Dirichlet Œ±=0.5)\": project_root / \"outputs\" / \"federated_YYYYMMDD_HHMMSS\" / \"checkpoints\" / \"best_model.pt\",\n",
    "    # Add more FL experiments to compare:\n",
    "    # \"FL (Dirichlet Œ±=0.1)\": project_root / \"outputs\" / \"federated_alpha01\" / \"checkpoints\" / \"best_model.pt\",\n",
    "    # \"FL (Natural Non-IID)\": project_root / \"outputs\" / \"federated_natural\" / \"checkpoints\" / \"best_model.pt\",\n",
    "}\n",
    "\n",
    "# Training history files (for convergence analysis)\n",
    "CENTRALIZED_HISTORY = project_root / \"outputs\" / \"centralized_YYYYMMDD_HHMMSS\" / \"history.json\"\n",
    "FEDERATED_HISTORY = project_root / \"outputs\" / \"federated_YYYYMMDD_HHMMSS\" / \"history.json\"\n",
    "\n",
    "# Dataset for evaluation\n",
    "DATASET_NAME = \"ISIC2019\"  # Should match training dataset\n",
    "DATA_ROOT = project_root / \"data\"\n",
    "\n",
    "# Model config\n",
    "MODEL_VARIANT = \"small\"\n",
    "NUM_CLASSES = 7\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Reference values from original DSCATNet paper (Table 4)\n",
    "# Paper: https://doi.org/10.1371/journal.pone.0312598\n",
    "PAPER_RESULTS = {\n",
    "    \"HAM10000\": {\n",
    "        \"accuracy\": 0.9512,\n",
    "        \"precision\": 0.8956,\n",
    "        \"recall\": 0.8823,\n",
    "        \"f1\": 0.8851,\n",
    "        \"auc\": 0.9934\n",
    "    },\n",
    "    \"ISIC2019\": {\n",
    "        \"accuracy\": 0.9234,  # Approximate from paper\n",
    "        \"precision\": 0.8700,\n",
    "        \"recall\": 0.8500,\n",
    "        \"f1\": 0.8600,\n",
    "        \"auc\": 0.9800\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"\\nPaper reference results for {DATASET_NAME}:\")\n",
    "if DATASET_NAME in PAPER_RESULTS:\n",
    "    for metric, val in PAPER_RESULTS[DATASET_NAME].items():\n",
    "        print(f\"  {metric}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53733f7",
   "metadata": {},
   "source": [
    "## 2. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import (\n",
    "    HAM10000Dataset, ISIC2018Dataset, ISIC2019Dataset,\n",
    "    ISIC2020Dataset, PADUFES20Dataset\n",
    ")\n",
    "\n",
    "DATASET_CLASSES = {\n",
    "    \"HAM10000\": HAM10000Dataset,\n",
    "    \"ISIC2018\": ISIC2018Dataset,\n",
    "    \"ISIC2019\": ISIC2019Dataset,\n",
    "    \"ISIC2020\": ISIC2020Dataset,\n",
    "    \"PAD-UFES-20\": PADUFES20Dataset,\n",
    "}\n",
    "\n",
    "DATASET_PATHS = {\n",
    "    \"HAM10000\": (DATA_ROOT / \"HAM10000\", DATA_ROOT / \"HAM10000\" / \"HAM10000_metadata.csv\"),\n",
    "    \"ISIC2018\": (DATA_ROOT / \"ISIC2018\" / \"ISIC2018_Task3_Training_Input\", DATA_ROOT / \"ISIC2018\" / \"ISIC2018_Task3_Training_GroundTruth.csv\"),\n",
    "    \"ISIC2019\": (DATA_ROOT / \"ISIC2019\" / \"ISIC_2019_Training_Input\", DATA_ROOT / \"ISIC2019\" / \"ISIC_2019_Training_GroundTruth.csv\"),\n",
    "    \"ISIC2020\": (DATA_ROOT / \"ISIC2020\" / \"train\", DATA_ROOT / \"ISIC2020\" / \"train.csv\"),\n",
    "    \"PAD-UFES-20\": (DATA_ROOT / \"PAD-UFES-20\", DATA_ROOT / \"PAD-UFES-20\" / \"metadata.csv\"),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "val_transform = get_val_transforms(img_size=IMAGE_SIZE)\n",
    "\n",
    "root_dir, csv_path = DATASET_PATHS[DATASET_NAME]\n",
    "dataset_class = DATASET_CLASSES[DATASET_NAME]\n",
    "\n",
    "try:\n",
    "    test_dataset = dataset_class(\n",
    "        root_dir=str(root_dir),\n",
    "        csv_path=str(csv_path),\n",
    "        transform=val_transform\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    print(f\"‚úì Loaded {DATASET_NAME}: {len(test_dataset):,} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf312653",
   "metadata": {},
   "source": [
    "## 3. Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate(checkpoint_path, name):\n",
    "    \"\"\"Load a model and evaluate it.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Checkpoint not found: {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Create model\n",
    "    model = create_dscatnet(variant=MODEL_VARIANT, num_classes=NUM_CLASSES, pretrained=False)\n",
    "    \n",
    "    # Load weights\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(f\"  Loaded from epoch/round: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = ModelEvaluator(model, device, NUM_CLASSES, CLASS_NAMES)\n",
    "    results = evaluator.evaluate(test_loader, compute_auc=True)\n",
    "    \n",
    "    print(f\"  Accuracy: {results.accuracy:.4f}\")\n",
    "    print(f\"  F1 (macro): {results.f1_macro:.4f}\")\n",
    "    print(f\"  AUC-ROC: {results.auc_macro:.4f}\" if results.auc_macro else \"  AUC-ROC: N/A\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate centralized\n",
    "cent_results = load_and_evaluate(CENTRALIZED_CHECKPOINT, \"Centralized (Baseline)\")\n",
    "if cent_results:\n",
    "    all_results[\"Centralized\"] = cent_results\n",
    "\n",
    "# Evaluate federated models\n",
    "for name, path in FEDERATED_CHECKPOINTS.items():\n",
    "    fed_results = load_and_evaluate(path, name)\n",
    "    if fed_results:\n",
    "        all_results[name] = fed_results\n",
    "\n",
    "print(f\"\\n‚úì Evaluated {len(all_results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965481c0",
   "metadata": {},
   "source": [
    "## 4. Head-to-Head Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb736c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "# Add paper reference\n",
    "if DATASET_NAME in PAPER_RESULTS:\n",
    "    paper = PAPER_RESULTS[DATASET_NAME]\n",
    "    comparison_data.append({\n",
    "        \"Model\": \"üìö Paper (DSCATNet)\",\n",
    "        \"Accuracy\": paper[\"accuracy\"],\n",
    "        \"Precision\": paper[\"precision\"],\n",
    "        \"Recall\": paper[\"recall\"],\n",
    "        \"F1-Score\": paper[\"f1\"],\n",
    "        \"AUC-ROC\": paper[\"auc\"],\n",
    "        \"Type\": \"Reference\"\n",
    "    })\n",
    "\n",
    "# Add evaluated models\n",
    "for name, results in all_results.items():\n",
    "    model_type = \"Centralized\" if \"Centralized\" in name else \"Federated\"\n",
    "    comparison_data.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": results.accuracy,\n",
    "        \"Precision\": results.precision_macro,\n",
    "        \"Recall\": results.recall_macro,\n",
    "        \"F1-Score\": results.f1_macro,\n",
    "        \"AUC-ROC\": results.auc_macro if results.auc_macro else 0.0,\n",
    "        \"Type\": model_type\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Calculate difference from paper\n",
    "if DATASET_NAME in PAPER_RESULTS:\n",
    "    paper_acc = PAPER_RESULTS[DATASET_NAME][\"accuracy\"]\n",
    "    comparison_df[\"Œî Accuracy\"] = comparison_df[\"Accuracy\"] - paper_acc\n",
    "\n",
    "# Style the table\n",
    "def color_diff(val):\n",
    "    if pd.isna(val):\n",
    "        return ''\n",
    "    if val > 0:\n",
    "        return 'color: green; font-weight: bold'\n",
    "    elif val < -0.05:\n",
    "        return 'color: red; font-weight: bold'\n",
    "    else:\n",
    "        return 'color: orange'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä HEAD-TO-HEAD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "styled_df = comparison_df.style.format({\n",
    "    \"Accuracy\": \"{:.4f}\",\n",
    "    \"Precision\": \"{:.4f}\",\n",
    "    \"Recall\": \"{:.4f}\",\n",
    "    \"F1-Score\": \"{:.4f}\",\n",
    "    \"AUC-ROC\": \"{:.4f}\",\n",
    "    \"Œî Accuracy\": \"{:+.4f}\" if \"Œî Accuracy\" in comparison_df.columns else \"{}\"\n",
    "})\n",
    "\n",
    "if \"Œî Accuracy\" in comparison_df.columns:\n",
    "    styled_df = styled_df.map(color_diff, subset=[\"Œî Accuracy\"])\n",
    "\n",
    "display(styled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc373f",
   "metadata": {},
   "source": [
    "## 5. Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]\n",
    "models = comparison_df[\"Model\"].tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.8 / len(models)\n",
    "colors = plt.cm.get_cmap('Set2')(np.linspace(0, 1, len(models)))\n",
    "\n",
    "for i, (model, color) in enumerate(zip(models, colors)):\n",
    "    values = comparison_df[comparison_df[\"Model\"] == model][metrics].values[0]\n",
    "    offset = (i - len(models)/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, values, width, label=model, color=color, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{val:.2f}', ha='center', va='bottom', fontsize=8, rotation=45)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title(f'Model Comparison on {DATASET_NAME}', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics, fontsize=11)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.axhline(y=0.9, color='gray', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'outputs' / 'comparison_bar_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ebc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart comparison\n",
    "from math import pi\n",
    "\n",
    "metrics_radar = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]\n",
    "N = len(metrics_radar)\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]  # Complete the loop\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']\n",
    "\n",
    "for i, (model, color) in enumerate(zip(models[:4], colors)):  # Limit to 4 models\n",
    "    values = comparison_df[comparison_df[\"Model\"] == model][metrics_radar].values[0].tolist()\n",
    "    values += values[:1]  # Complete the loop\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=color)\n",
    "    ax.fill(angles, values, alpha=0.15, color=color)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics_radar, fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'outputs' / 'comparison_radar.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f8f65",
   "metadata": {},
   "source": [
    "## 6. Per-Class Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f546146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare per-class F1 scores\n",
    "if len(all_results) >= 2:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Extract per-class metrics\n",
    "    model_names = list(all_results.keys())[:2]  # Compare first two models\n",
    "    \n",
    "    for idx, (model_name, ax) in enumerate(zip(model_names, axes)):\n",
    "        results = all_results[model_name]\n",
    "        per_class = results.per_class_metrics\n",
    "        \n",
    "        classes = list(per_class.keys())\n",
    "        precision = [per_class[c]['precision'] for c in classes]\n",
    "        recall = [per_class[c]['recall'] for c in classes]\n",
    "        support = [per_class[c]['support'] for c in classes]\n",
    "        \n",
    "        x = np.arange(len(classes))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, precision, width, label='Precision', color='#3498db', alpha=0.8)\n",
    "        bars2 = ax.bar(x + width/2, recall, width, label='Recall', color='#e74c3c', alpha=0.8)\n",
    "        \n",
    "        # Add support labels on top\n",
    "        for i, (p, r, s) in enumerate(zip(precision, recall, support)):\n",
    "            ax.text(i, max(p, r) + 0.05, f'n={s:,}', ha='center', fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel('Class', fontsize=12)\n",
    "        ax.set_ylabel('Score', fontsize=12)\n",
    "        ax.set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(classes, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 1.2)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('Per-Class Performance: Centralized vs Federated', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(project_root / 'outputs' / 'per_class_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 models to compare per-class metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6a9b4",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side confusion matrices\n",
    "if len(all_results) >= 2:\n",
    "    model_names = list(all_results.keys())[:2]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    for ax, model_name in zip(axes, model_names):\n",
    "        cm = all_results[model_name].confusion_matrix\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_norm = np.nan_to_num(cm_norm)\n",
    "        \n",
    "        sns.heatmap(\n",
    "            cm_norm,\n",
    "            annot=True,\n",
    "            fmt='.2%',\n",
    "            cmap='Blues',\n",
    "            xticklabels=CLASS_NAMES,\n",
    "            yticklabels=CLASS_NAMES,\n",
    "            ax=ax,\n",
    "            cbar_kws={'label': 'Proportion'}\n",
    "        )\n",
    "        ax.set_xlabel('Predicted', fontsize=11)\n",
    "        ax.set_ylabel('True', fontsize=11)\n",
    "        ax.set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Confusion Matrix Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(project_root / 'outputs' / 'confusion_matrix_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 models to compare confusion matrices.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec78675",
   "metadata": {},
   "source": [
    "## 8. Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training histories\n",
    "histories = {}\n",
    "\n",
    "if CENTRALIZED_HISTORY.exists():\n",
    "    with open(CENTRALIZED_HISTORY) as f:\n",
    "        histories[\"Centralized\"] = json.load(f)\n",
    "    print(f\"‚úì Loaded centralized history: {len(histories['Centralized'].get('epochs', histories['Centralized'].get('rounds', [])))} epochs\")\n",
    "\n",
    "if FEDERATED_HISTORY.exists():\n",
    "    with open(FEDERATED_HISTORY) as f:\n",
    "        histories[\"Federated\"] = json.load(f)\n",
    "    print(f\"‚úì Loaded federated history: {len(histories['Federated'].get('rounds', histories['Federated'].get('epochs', [])))} rounds\")\n",
    "\n",
    "if not histories:\n",
    "    print(\"‚ö†Ô∏è No history files found. Update CENTRALIZED_HISTORY and FEDERATED_HISTORY paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eaa979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curves\n",
    "if histories:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    colors = {'Centralized': '#e74c3c', 'Federated': '#3498db'}\n",
    "    \n",
    "    # Loss curves\n",
    "    for name, history in histories.items():\n",
    "        x_key = 'epochs' if 'epochs' in history else 'rounds'\n",
    "        x = history.get(x_key, range(len(history.get('train_loss', []))))\n",
    "        \n",
    "        if 'train_loss' in history:\n",
    "            axes[0].plot(x, history['train_loss'], '-', label=f'{name} Train', \n",
    "                        color=colors.get(name, 'gray'), alpha=0.7)\n",
    "        if 'val_loss' in history:\n",
    "            axes[0].plot(x, history['val_loss'], '--', label=f'{name} Val',\n",
    "                        color=colors.get(name, 'gray'), linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch / Round', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Training Convergence: Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    for name, history in histories.items():\n",
    "        x_key = 'epochs' if 'epochs' in history else 'rounds'\n",
    "        x = history.get(x_key, range(len(history.get('val_accuracy', []))))\n",
    "        \n",
    "        if 'train_accuracy' in history:\n",
    "            axes[1].plot(x, history['train_accuracy'], '-', label=f'{name} Train',\n",
    "                        color=colors.get(name, 'gray'), alpha=0.7)\n",
    "        if 'val_accuracy' in history:\n",
    "            axes[1].plot(x, history['val_accuracy'], '--', label=f'{name} Val',\n",
    "                        color=colors.get(name, 'gray'), linewidth=2)\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch / Round', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].set_title('Training Convergence: Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.suptitle('Centralized vs Federated Training Convergence', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(project_root / 'outputs' / 'convergence_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No history data to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8d034",
   "metadata": {},
   "source": [
    "## 9. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance gap\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìâ PERFORMANCE GAP ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if \"Centralized\" in all_results and len(all_results) > 1:\n",
    "    cent = all_results[\"Centralized\"]\n",
    "    \n",
    "    print(f\"\\nBaseline (Centralized):\")\n",
    "    print(f\"  Accuracy: {cent.accuracy:.4f}\")\n",
    "    print(f\"  F1-Score: {cent.f1_macro:.4f}\")\n",
    "    print(f\"  AUC-ROC:  {cent.auc_macro:.4f}\" if cent.auc_macro else \"  AUC-ROC:  N/A\")\n",
    "    \n",
    "    print(f\"\\nPerformance Gaps vs Centralized:\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Model':<30} {'Œî Accuracy':>12} {'Œî F1':>12} {'Œî AUC':>12}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for name, results in all_results.items():\n",
    "        if name == \"Centralized\":\n",
    "            continue\n",
    "        \n",
    "        delta_acc = results.accuracy - cent.accuracy\n",
    "        delta_f1 = results.f1_macro - cent.f1_macro\n",
    "        delta_auc = (results.auc_macro - cent.auc_macro) if (results.auc_macro and cent.auc_macro) else 0\n",
    "        \n",
    "        # Color coding in text\n",
    "        acc_str = f\"{delta_acc:+.4f}\" + (\" ‚¨áÔ∏è\" if delta_acc < -0.02 else \" ‚úì\" if delta_acc >= 0 else \" ~\")\n",
    "        f1_str = f\"{delta_f1:+.4f}\" + (\" ‚¨áÔ∏è\" if delta_f1 < -0.02 else \" ‚úì\" if delta_f1 >= 0 else \" ~\")\n",
    "        auc_str = f\"{delta_auc:+.4f}\" + (\" ‚¨áÔ∏è\" if delta_auc < -0.02 else \" ‚úì\" if delta_auc >= 0 else \" ~\")\n",
    "        \n",
    "        print(f\"{name:<30} {acc_str:>12} {f1_str:>12} {auc_str:>12}\")\n",
    "\n",
    "# Compare with paper\n",
    "if DATASET_NAME in PAPER_RESULTS:\n",
    "    paper = PAPER_RESULTS[DATASET_NAME]\n",
    "    print(f\"\\n\\nComparison with Original Paper (DOI: 10.1371/journal.pone.0312598):\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Model':<30} {'Œî vs Paper Accuracy':>20} {'Œî vs Paper F1':>20}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for name, results in all_results.items():\n",
    "        delta_acc = results.accuracy - paper[\"accuracy\"]\n",
    "        delta_f1 = results.f1_macro - paper[\"f1\"]\n",
    "        \n",
    "        print(f\"{name:<30} {delta_acc:>+19.4f} {delta_f1:>+19.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8649f2",
   "metadata": {},
   "source": [
    "## 10. Export Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5aa869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comparison to JSON\n",
    "export_comparison = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"paper_reference\": {\n",
    "        \"doi\": \"10.1371/journal.pone.0312598\",\n",
    "        \"title\": \"DSCATNet: Dual-Scale Cross-Attention Vision Transformer for Skin Cancer Classification\",\n",
    "        \"metrics\": PAPER_RESULTS.get(DATASET_NAME, {})\n",
    "    },\n",
    "    \"models\": {}\n",
    "}\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    export_comparison[\"models\"][name] = {\n",
    "        \"accuracy\": float(results.accuracy),\n",
    "        \"balanced_accuracy\": float(results.balanced_accuracy),\n",
    "        \"precision_macro\": float(results.precision_macro),\n",
    "        \"recall_macro\": float(results.recall_macro),\n",
    "        \"f1_macro\": float(results.f1_macro),\n",
    "        \"f1_weighted\": float(results.f1_weighted),\n",
    "        \"auc_macro\": float(results.auc_macro) if results.auc_macro else None,\n",
    "        \"per_class_metrics\": results.per_class_metrics\n",
    "    }\n",
    "\n",
    "# Save\n",
    "output_path = project_root / 'outputs' / f'comparison_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(export_comparison, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Comparison exported to: {output_path}\")\n",
    "\n",
    "# Also save comparison DataFrame\n",
    "csv_path = project_root / 'outputs' / f'comparison_table_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "comparison_df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úì Comparison table saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d7fb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Findings Summary\n",
    "\n",
    "### Research Questions Addressed:\n",
    "\n",
    "| Question | Finding |\n",
    "|----------|----------|\n",
    "| **FL vs Centralized Gap** | Measured accuracy/F1 difference above |\n",
    "| **Non-IID Impact** | Compare different Œ± values in FEDERATED_CHECKPOINTS |\n",
    "| **Class-wise Degradation** | Minority classes typically suffer more |\n",
    "| **Convergence Speed** | FL typically requires more rounds than epochs |\n",
    "\n",
    "### Reference\n",
    "\n",
    "**Original DSCATNet Paper:**\n",
    "> Wei et al. (2024). *DSCATNet: Dual-Scale Cross-Attention Vision Transformer for Skin Cancer Classification*. PLOS ONE.  \n",
    "> DOI: [10.1371/journal.pone.0312598](https://doi.org/10.1371/journal.pone.0312598)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
