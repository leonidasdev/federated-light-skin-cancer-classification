{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9446eece",
   "metadata": {},
   "source": [
    "# Dataset Exploration Notebook\n",
    "\n",
    "**Federated Learning for Skin Cancer Classification with DSCATNet**\n",
    "\n",
    "This notebook provides comprehensive dataset exploration and verification, including:\n",
    "\n",
    "- **Dataset Verification**: Check all datasets are properly downloaded\n",
    "- **Class Distribution Analysis**: Understand class imbalances across FL clients\n",
    "- **Image Statistics**: Analyze dimensions, aspect ratios, and pixel values\n",
    "- **Non-IID Visualization**: Visualize data heterogeneity across clients\n",
    "- **Preprocessing Pipeline**: Test and visualize augmentation transforms\n",
    "- **Sample Visualization**: Display sample images from each dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29695c97",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a3097",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Data directory\n",
    "DATA_ROOT = project_root / \"data\"\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "DATA_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# Define unified class names (7-class scheme)\n",
    "UNIFIED_CLASSES = [\n",
    "    'Actinic Keratosis',\n",
    "    'Basal Cell Carcinoma', \n",
    "    'Benign Keratosis',\n",
    "    'Dermatofibroma',\n",
    "    'Melanoma',\n",
    "    'Melanocytic Nevus',\n",
    "    'Vascular Lesion'\n",
    "]\n",
    "\n",
    "# Class abbreviations\n",
    "CLASS_ABBREV = ['AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC']\n",
    "\n",
    "# Define image directories for each client\n",
    "IMAGE_DIRS = {\n",
    "    'HAM10000': DATA_ROOT / 'HAM10000' / 'HAM10000_images_part_1',\n",
    "    'ISIC2018': DATA_ROOT / 'ISIC2018' / 'ISIC2018_Task3_Training_Input',\n",
    "    'ISIC2019': DATA_ROOT / 'ISIC2019' / 'ISIC_2019_Training_Input',\n",
    "    'ISIC2020': DATA_ROOT / 'ISIC2020' / 'train'\n",
    "}\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Exists: {DATA_ROOT.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47bc84c",
   "metadata": {},
   "source": [
    "## 3. Dataset Verification\n",
    "\n",
    "First, let's verify that all datasets are properly downloaded and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae405435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.verify import DatasetVerifier\n",
    "\n",
    "# Run verification\n",
    "verifier = DatasetVerifier(str(DATA_ROOT))\n",
    "results = verifier.verify_all(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary = verifier.get_summary_stats()\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Valid datasets: {summary['valid_datasets']}/4\")\n",
    "print(f\"Total images: {summary['total_images']:,}\")\n",
    "print(\"\\nImages per dataset:\")\n",
    "for name, count in summary['images_per_dataset'].items():\n",
    "    print(f\"  {name}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f42f5",
   "metadata": {},
   "source": [
    "## 4. Class Distribution Analysis\n",
    "\n",
    "Understanding class distributions is crucial for federated learning, especially when dealing with non-IID data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31693a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect class distributions from all datasets\n",
    "distributions = {}\n",
    "\n",
    "for name, result in results.items():\n",
    "    if result['class_distribution']:\n",
    "        distributions[name] = result['class_distribution']\n",
    "\n",
    "# Display raw distributions\n",
    "for name, dist in distributions.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for cls, count in dist.items():\n",
    "        print(f\"  {cls}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = sns.color_palette('husl', 8)\n",
    "\n",
    "for idx, (name, dist) in enumerate(distributions.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    classes = list(dist.keys())\n",
    "    counts = list(dist.values())\n",
    "    \n",
    "    bars = ax.bar(classes, counts, color=colors[:len(classes)])\n",
    "    ax.set_title(f'{name} (Client {idx+1})', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                f'{count:,}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.suptitle('Class Distribution Across FL Clients (Non-IID)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_ROOT.parent / 'experiments' / 'class_distribution.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68606a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unified distribution comparison table\n",
    "def map_to_unified(dist, dataset_name):\n",
    "    \"\"\"Map dataset-specific labels to unified 7-class scheme.\"\"\"\n",
    "    unified = {c: 0 for c in CLASS_ABBREV}\n",
    "    \n",
    "    mapping = {\n",
    "        # HAM10000 / ISIC2018 mapping\n",
    "        'akiec': 'AK', 'AKIEC': 'AK',\n",
    "        'bcc': 'BCC', 'BCC': 'BCC',\n",
    "        'bkl': 'BKL', 'BKL': 'BKL',\n",
    "        'df': 'DF', 'DF': 'DF',\n",
    "        'mel': 'MEL', 'MEL': 'MEL',\n",
    "        'nv': 'NV', 'NV': 'NV',\n",
    "        'vasc': 'VASC', 'VASC': 'VASC',\n",
    "        # ISIC2019 additional\n",
    "        'AK': 'AK',\n",
    "        'SCC': 'BCC',  # Map SCC to BCC (carcinomas)\n",
    "        # ISIC2020 binary\n",
    "        'benign': 'NV',\n",
    "        'malignant': 'MEL'\n",
    "    }\n",
    "    \n",
    "    for label, count in dist.items():\n",
    "        unified_label = mapping.get(label, None)\n",
    "        if unified_label:\n",
    "            unified[unified_label] += count\n",
    "    \n",
    "    return unified\n",
    "\n",
    "# Create comparison DataFrame\n",
    "unified_dists = {}\n",
    "for name, dist in distributions.items():\n",
    "    unified_dists[name] = map_to_unified(dist, name)\n",
    "\n",
    "df_dist = pd.DataFrame(unified_dists).T\n",
    "df_dist['Total'] = df_dist.sum(axis=1)\n",
    "\n",
    "print(\"\\nUnified Class Distribution (7 Classes):\")\n",
    "print(df_dist.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize unified distribution as heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Normalize by row (percentage within each client)\n",
    "df_pct = df_dist.drop('Total', axis=1)\n",
    "df_pct = df_pct.div(df_pct.sum(axis=1), axis=0) * 100\n",
    "\n",
    "sns.heatmap(df_pct, annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "            ax=ax, cbar_kws={'label': 'Percentage (%)'})\n",
    "ax.set_title('Class Distribution per Client (% within each client)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Dataset (FL Client)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- HAM10000/ISIC2018: Similar balanced 7-class distributions\")\n",
    "print(\"- ISIC2019: Includes SCC, more diverse\")\n",
    "print(\"- ISIC2020: Binary only â†’ strong non-IID with other clients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52326ae0",
   "metadata": {},
   "source": [
    "## 5. Image Statistics\n",
    "\n",
    "Analyze image dimensions, aspect ratios, and pixel statistics across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image_stats(image_dir, n_samples=500):\n",
    "    \"\"\"Sample images and compute statistics.\"\"\"\n",
    "    image_dir = Path(image_dir)\n",
    "    if not image_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    images = list(image_dir.glob(\"*.jpg\"))[:n_samples]\n",
    "    \n",
    "    stats = {\n",
    "        'widths': [],\n",
    "        'heights': [],\n",
    "        'aspect_ratios': [],\n",
    "        'mean_r': [],\n",
    "        'mean_g': [],\n",
    "        'mean_b': []\n",
    "    }\n",
    "    \n",
    "    for img_path in images:\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                stats['widths'].append(img.width)\n",
    "                stats['heights'].append(img.height)\n",
    "                stats['aspect_ratios'].append(img.width / img.height)\n",
    "                \n",
    "                # Sample pixel values (resize for efficiency)\n",
    "                img_small = img.resize((64, 64))\n",
    "                arr = np.array(img_small)\n",
    "                if len(arr.shape) == 3:\n",
    "                    stats['mean_r'].append(arr[:,:,0].mean())\n",
    "                    stats['mean_g'].append(arr[:,:,1].mean())\n",
    "                    stats['mean_b'].append(arr[:,:,2].mean())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Compute stats using IMAGE_DIRS from configuration\n",
    "all_stats = {}\n",
    "for name, path in IMAGE_DIRS.items():\n",
    "    print(f\"Sampling {name}...\")\n",
    "    stats = sample_image_stats(path, n_samples=300)\n",
    "    if stats and stats['widths']:\n",
    "        all_stats[name] = stats\n",
    "        print(f\"  Sampled {len(stats['widths'])} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image dimensions\n",
    "if all_stats:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Width distribution\n",
    "    for name, stats in all_stats.items():\n",
    "        axes[0].hist(stats['widths'], bins=20, alpha=0.5, label=name)\n",
    "    axes[0].set_xlabel('Width (pixels)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Image Width Distribution')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Height distribution\n",
    "    for name, stats in all_stats.items():\n",
    "        axes[1].hist(stats['heights'], bins=20, alpha=0.5, label=name)\n",
    "    axes[1].set_xlabel('Height (pixels)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Image Height Distribution')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Aspect ratio\n",
    "    for name, stats in all_stats.items():\n",
    "        axes[2].hist(stats['aspect_ratios'], bins=20, alpha=0.5, label=name)\n",
    "    axes[2].set_xlabel('Aspect Ratio (W/H)')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    axes[2].set_title('Aspect Ratio Distribution')\n",
    "    axes[2].axvline(x=1.0, color='red', linestyle='--', label='Square')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No image statistics available. Please download datasets first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e66934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color distribution (important for normalization)\n",
    "if all_stats:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    channels = ['mean_r', 'mean_g', 'mean_b']\n",
    "    titles = ['Red Channel Mean', 'Green Channel Mean', 'Blue Channel Mean']\n",
    "    \n",
    "    for ax, channel, title in zip(axes, channels, titles):\n",
    "        for name, stats in all_stats.items():\n",
    "            if stats[channel]:\n",
    "                ax.hist(stats[channel], bins=30, alpha=0.5, label=name)\n",
    "        ax.set_xlabel('Pixel Value (0-255)')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print mean values per dataset\n",
    "    print(\"\\nMean RGB values per dataset:\")\n",
    "    for name, stats in all_stats.items():\n",
    "        if stats['mean_r']:\n",
    "            r = np.mean(stats['mean_r'])\n",
    "            g = np.mean(stats['mean_g'])\n",
    "            b = np.mean(stats['mean_b'])\n",
    "            print(f\"  {name}: R={r:.1f}, G={g:.1f}, B={b:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273808f",
   "metadata": {},
   "source": [
    "## 6. Non-IID Visualization\n",
    "\n",
    "Visualize the non-IID nature of the federated learning setup with different data distributions across clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff0a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-IID visualization\n",
    "if distributions:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 1. Stacked bar chart showing absolute numbers\n",
    "    ax1 = axes[0]\n",
    "    clients = list(distributions.keys())\n",
    "    \n",
    "    # Get all unique classes\n",
    "    all_classes = set()\n",
    "    for dist in distributions.values():\n",
    "        all_classes.update(dist.keys())\n",
    "    all_classes = sorted(all_classes)\n",
    "    \n",
    "    # Create stacked bar\n",
    "    bottom = np.zeros(len(clients))\n",
    "    for cls in all_classes:\n",
    "        values = [distributions[c].get(cls, 0) for c in clients]\n",
    "        ax1.bar(clients, values, bottom=bottom, label=cls)\n",
    "        bottom += values\n",
    "    \n",
    "    ax1.set_xlabel('FL Client (Dataset)')\n",
    "    ax1.set_ylabel('Number of Images')\n",
    "    ax1.set_title('Data Quantity per Client (Non-IID)')\n",
    "    ax1.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Radar chart for class presence\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Create presence matrix (binary: has class or not)\n",
    "    presence = []\n",
    "    for client in clients:\n",
    "        row = [1 if distributions[client].get(cls, 0) > 0 else 0 \n",
    "               for cls in all_classes]\n",
    "        presence.append(row)\n",
    "    \n",
    "    presence_df = pd.DataFrame(presence, index=clients, columns=all_classes)\n",
    "    sns.heatmap(presence_df, annot=True, cmap='RdYlGn', ax=ax2,\n",
    "                cbar_kws={'label': 'Class Present'})\n",
    "    ax2.set_title('Class Presence per Client')\n",
    "    ax2.set_xlabel('Class')\n",
    "    ax2.set_ylabel('Client')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nNon-IID Analysis:\")\n",
    "    print(\"- Each client has different class distributions\")\n",
    "    print(\"- ISIC2020 only has 2 classes (extreme non-IID)\")\n",
    "    print(\"- Data quantity varies significantly (10k to 33k)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6620f9",
   "metadata": {},
   "source": [
    "## 7. Preprocessing Pipeline Test\n",
    "\n",
    "Test the standardized preprocessing pipeline with different augmentation levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preprocessing import (\n",
    "    get_train_transforms,\n",
    "    get_val_transforms,\n",
    "    IMAGENET_MEAN,\n",
    "    IMAGENET_STD\n",
    ")\n",
    "\n",
    "# Create transforms\n",
    "train_transform = get_train_transforms(img_size=224, augmentation_level='medium')\n",
    "val_transform = get_val_transforms(img_size=224)\n",
    "\n",
    "print(\"Training transforms:\")\n",
    "print(train_transform)\n",
    "print(\"\\nValidation transforms:\")\n",
    "print(val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ade9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(image_path, n_augmentations=6):\n",
    "    \"\"\"Visualize multiple augmentations of the same image.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(img_array)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Validation (no augmentation)\n",
    "    val_result = val_transform(image=img_array)\n",
    "    val_img = val_result['image'].permute(1, 2, 0).numpy()\n",
    "    # Denormalize for visualization\n",
    "    val_img = val_img * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)\n",
    "    val_img = np.clip(val_img, 0, 1)\n",
    "    axes[1].imshow(val_img)\n",
    "    axes[1].set_title('Validation (224x224)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Training augmentations\n",
    "    for i in range(2, 8):\n",
    "        aug_result = train_transform(image=img_array)\n",
    "        aug_img = aug_result['image'].permute(1, 2, 0).numpy()\n",
    "        aug_img = aug_img * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)\n",
    "        aug_img = np.clip(aug_img, 0, 1)\n",
    "        axes[i].imshow(aug_img)\n",
    "        axes[i].set_title(f'Augmentation {i-1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Preprocessing Pipeline Visualization', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Find a sample image using IMAGE_DIRS from configuration\n",
    "sample_found = False\n",
    "for img_dir in IMAGE_DIRS.values():\n",
    "    if img_dir.exists():\n",
    "        images = list(img_dir.glob(\"*.jpg\"))\n",
    "        if images:\n",
    "            visualize_augmentations(images[0])\n",
    "            sample_found = True\n",
    "            break\n",
    "\n",
    "if not sample_found:\n",
    "    print(\"No sample images found. Please download datasets first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10364361",
   "metadata": {},
   "source": [
    "## 8. Sample Visualization\n",
    "\n",
    "Display sample images from each dataset/client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42063e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples_per_client(n_samples=4):\n",
    "    \"\"\"Display sample images from each FL client.\"\"\"\n",
    "    fig, axes = plt.subplots(4, n_samples, figsize=(3*n_samples, 12))\n",
    "    \n",
    "    client_names = ['HAM10000 (Client 1)', 'ISIC2018 (Client 2)', \n",
    "                    'ISIC2019 (Client 3)', 'ISIC2020 (Client 4)']\n",
    "    \n",
    "    for row, (name, img_dir) in enumerate(IMAGE_DIRS.items()):\n",
    "        if not img_dir.exists():\n",
    "            for col in range(n_samples):\n",
    "                axes[row, col].text(0.5, 0.5, 'Not Found', \n",
    "                                   ha='center', va='center')\n",
    "                axes[row, col].axis('off')\n",
    "            axes[row, 0].set_ylabel(client_names[row], fontsize=12)\n",
    "            continue\n",
    "            \n",
    "        images = list(img_dir.glob(\"*.jpg\"))[:n_samples]\n",
    "        \n",
    "        for col, img_path in enumerate(images):\n",
    "            img = Image.open(img_path)\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].axis('off')\n",
    "            if col == 0:\n",
    "                axes[row, col].set_ylabel(client_names[row], fontsize=12)\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each FL Client', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_samples_per_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694963f",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "Generate a summary report for the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary table for thesis\n",
    "summary_data = []\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    row = {\n",
    "        'Client': idx + 1,\n",
    "        'Dataset': name,\n",
    "        'Images': result['total_images'],\n",
    "        'Classes': len(result['class_distribution']) if result['class_distribution'] else 0,\n",
    "        'Status': 'OK' if result['valid'] else 'FAIL'\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SUMMARY FOR THESIS\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LaTeX table format\n",
    "print(\"\\n\\nLaTeX Table:\")\n",
    "print(\"\\\\begin{table}[h]\")\n",
    "print(\"\\\\centering\")\n",
    "print(\"\\\\caption{Dermoscopy Datasets Used in Federated Learning Experiments}\")\n",
    "print(\"\\\\begin{tabular}{|c|l|r|c|}\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"\\\\textbf{Client} & \\\\textbf{Dataset} & \\\\textbf{Images} & \\\\textbf{Classes} \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "for _, row in summary_df.iterrows():\n",
    "    print(f\"{row['Client']} & {row['Dataset']} & {row['Images']:,} & {row['Classes']} \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"\\\\end{tabular}\")\n",
    "print(\"\\\\label{tab:datasets}\")\n",
    "print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97096195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. Download missing datasets (see data/download.py instructions)\n",
    "2. Verify all datasets are correctly organized\n",
    "3. Run preprocessing pipeline tests\n",
    "4. Create IID vs Non-IID split configurations\n",
    "5. Test DataLoader for each client\n",
    "6. Proceed to model training with run_fl.py or run_experiment.py\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
