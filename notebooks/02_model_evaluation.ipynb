{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8693b8",
   "metadata": {},
   "source": [
    "# Model Evaluation Notebook\n",
    "\n",
    "**Federated Learning for Skin Cancer Classification with DSCATNet**\n",
    "\n",
    "This notebook provides comprehensive evaluation tools for trained models, including:\n",
    "\n",
    "- **Performance Metrics**: Accuracy, Balanced Accuracy, F1, AUC-ROC\n",
    "- **Confusion Matrix**: Visual analysis of predictions\n",
    "- **Per-Class Analysis**: Sensitivity/Specificity for each skin lesion type\n",
    "- **ROC Curves**: Multi-class ROC curves with AUC\n",
    "- **Prediction Confidence**: Distribution analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20fd98",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Scikit-learn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, auc,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project imports\n",
    "from src.models.dscatnet import create_dscatnet\n",
    "from src.data.datasets import UNIFIED_CLASSES\n",
    "from src.data.preprocessing import get_val_transforms\n",
    "from src.evaluation.metrics import ModelEvaluator, EvaluationResults\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de534cd9",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure the evaluation by setting the checkpoint path and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these paths for your experiment\n",
    "# =============================================================================\n",
    "\n",
    "# Path to trained model checkpoint\n",
    "CHECKPOINT_PATH = project_root / \"outputs\" / \"federated_YYYYMMDD_HHMMSS\" / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "# Dataset for evaluation\n",
    "DATASET_NAME = \"ISIC2019\"  # Options: HAM10000, ISIC2018, ISIC2019, ISIC2020, PAD-UFES-20\n",
    "DATA_ROOT = project_root / \"data\"\n",
    "\n",
    "# Model configuration (must match training)\n",
    "MODEL_VARIANT = \"small\"  # tiny, small, base\n",
    "NUM_CLASSES = 7\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Evaluation settings\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = list(UNIFIED_CLASSES.values())\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c73791",
   "metadata": {},
   "source": [
    "## 3. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6acfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = create_dscatnet(\n",
    "    variant=MODEL_VARIANT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    print(f\"Loading checkpoint: {CHECKPOINT_PATH}\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        epoch = checkpoint.get(\"epoch\", \"unknown\")\n",
    "        metrics = checkpoint.get(\"metrics\", {})\n",
    "        print(f\"Loaded from epoch/round: {epoch}\")\n",
    "        if metrics:\n",
    "            print(f\"Checkpoint metrics: {metrics}\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"Loaded raw state dict\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Checkpoint not found: {CHECKPOINT_PATH}\")\n",
    "    print(\"Please update CHECKPOINT_PATH to point to your trained model.\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Parameters: {total_params:,} total, {trainable_params:,} trainable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from src.data.datasets import (\n",
    "    HAM10000Dataset, ISIC2018Dataset, ISIC2019Dataset, \n",
    "    ISIC2020Dataset, PADUFES20Dataset\n",
    ")\n",
    "\n",
    "# Dataset mapping\n",
    "DATASET_CLASSES = {\n",
    "    \"HAM10000\": HAM10000Dataset,\n",
    "    \"ISIC2018\": ISIC2018Dataset,\n",
    "    \"ISIC2019\": ISIC2019Dataset,\n",
    "    \"ISIC2020\": ISIC2020Dataset,\n",
    "    \"PAD-UFES-20\": PADUFES20Dataset,\n",
    "}\n",
    "\n",
    "# Dataset paths (adjust if different)\n",
    "DATASET_PATHS = {\n",
    "    \"HAM10000\": {\n",
    "        \"root_dir\": DATA_ROOT / \"HAM10000\",\n",
    "        \"csv_path\": DATA_ROOT / \"HAM10000\" / \"HAM10000_metadata.csv\"\n",
    "    },\n",
    "    \"ISIC2018\": {\n",
    "        \"root_dir\": DATA_ROOT / \"ISIC2018\" / \"ISIC2018_Task3_Training_Input\",\n",
    "        \"csv_path\": DATA_ROOT / \"ISIC2018\" / \"ISIC2018_Task3_Training_GroundTruth.csv\"\n",
    "    },\n",
    "    \"ISIC2019\": {\n",
    "        \"root_dir\": DATA_ROOT / \"ISIC2019\" / \"ISIC_2019_Training_Input\",\n",
    "        \"csv_path\": DATA_ROOT / \"ISIC2019\" / \"ISIC_2019_Training_GroundTruth.csv\"\n",
    "    },\n",
    "    \"ISIC2020\": {\n",
    "        \"root_dir\": DATA_ROOT / \"ISIC2020\" / \"train\",\n",
    "        \"csv_path\": DATA_ROOT / \"ISIC2020\" / \"train.csv\"\n",
    "    },\n",
    "    \"PAD-UFES-20\": {\n",
    "        \"root_dir\": DATA_ROOT / \"PAD-UFES-20\",\n",
    "        \"csv_path\": DATA_ROOT / \"PAD-UFES-20\" / \"metadata.csv\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Validation transforms\n",
    "val_transform = get_val_transforms(img_size=IMAGE_SIZE)\n",
    "\n",
    "# Load dataset\n",
    "dataset_class = DATASET_CLASSES[DATASET_NAME]\n",
    "paths = DATASET_PATHS[DATASET_NAME]\n",
    "\n",
    "try:\n",
    "    test_dataset = dataset_class(\n",
    "        root_dir=str(paths[\"root_dir\"]),\n",
    "        csv_path=str(paths[\"csv_path\"]),\n",
    "        transform=val_transform\n",
    "    )\n",
    "    print(f\"‚úì Loaded {DATASET_NAME}: {len(test_dataset)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed to load {DATASET_NAME}: {e}\")\n",
    "    print(\"\\nPlease check that the dataset is properly downloaded and extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb62017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"DataLoader: {len(test_loader)} batches of size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f09c8",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    class_names=CLASS_NAMES\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running evaluation...\")\n",
    "results = evaluator.evaluate(test_loader, compute_auc=True)\n",
    "print(\"‚úì Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47402b0",
   "metadata": {},
   "source": [
    "## 5. Performance Metrics Summary\n",
    "\n",
    "### Key Performance Indicators (KPIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics summary DataFrame\n",
    "metrics_summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Accuracy\",\n",
    "        \"Balanced Accuracy\",\n",
    "        \"Precision (macro)\",\n",
    "        \"Recall (macro)\",\n",
    "        \"F1-Score (macro)\",\n",
    "        \"F1-Score (weighted)\",\n",
    "        \"AUC-ROC (macro)\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        results.accuracy,\n",
    "        results.balanced_accuracy,\n",
    "        results.precision_macro,\n",
    "        results.recall_macro,\n",
    "        results.f1_macro,\n",
    "        results.f1_weighted,\n",
    "        results.auc_macro if results.auc_macro else 0.0\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Style the DataFrame\n",
    "def highlight_kpi(val):\n",
    "    if val >= 0.90:\n",
    "        return 'background-color: #90EE90'  # Light green\n",
    "    elif val >= 0.80:\n",
    "        return 'background-color: #FFFFE0'  # Light yellow\n",
    "    elif val >= 0.70:\n",
    "        return 'background-color: #FFD700'  # Gold\n",
    "    else:\n",
    "        return 'background-color: #FFB6C1'  # Light red\n",
    "\n",
    "styled_metrics = metrics_summary.style.format({\"Value\": \"{:.4f}\"}).applymap(\n",
    "    highlight_kpi, subset=[\"Value\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "display(styled_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31838a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual KPI Dashboard\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "kpis = [\n",
    "    (\"Accuracy\", results.accuracy),\n",
    "    (\"Balanced\\nAccuracy\", results.balanced_accuracy),\n",
    "    (\"Precision\", results.precision_macro),\n",
    "    (\"Recall\", results.recall_macro),\n",
    "    (\"F1 (macro)\", results.f1_macro),\n",
    "    (\"F1 (weighted)\", results.f1_weighted),\n",
    "    (\"AUC-ROC\", results.auc_macro if results.auc_macro else 0.0),\n",
    "    (\"Total\\nSamples\", len(results.labels))\n",
    "]\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#9b59b6', '#e74c3c', \n",
    "          '#f39c12', '#1abc9c', '#e67e22', '#34495e']\n",
    "\n",
    "for idx, ((name, value), color) in enumerate(zip(kpis, colors)):\n",
    "    ax = axes.flat[idx]\n",
    "    \n",
    "    if idx < 7:  # Metrics (0-1 scale)\n",
    "        # Create a gauge-like visualization\n",
    "        ax.barh([0], [value], color=color, height=0.5, alpha=0.8)\n",
    "        ax.barh([0], [1], color='lightgray', height=0.5, alpha=0.3)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_yticks([])\n",
    "        ax.text(0.5, 0.7, f\"{value:.2%}\", ha='center', va='bottom', \n",
    "                fontsize=24, fontweight='bold', transform=ax.transAxes)\n",
    "    else:  # Sample count\n",
    "        ax.text(0.5, 0.5, f\"{int(value):,}\", ha='center', va='center',\n",
    "                fontsize=28, fontweight='bold', color=color, transform=ax.transAxes)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    ax.set_title(name, fontsize=12, fontweight='bold', pad=10)\n",
    "    if idx < 7:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "\n",
    "plt.suptitle(f\"Model Performance Dashboard - {DATASET_NAME}\", fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbe361",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c85c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix (normalized and absolute)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_norm = results.confusion_matrix.astype('float') / results.confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_norm, \n",
    "    annot=True, \n",
    "    fmt='.2%', \n",
    "    cmap='Blues',\n",
    "    xticklabels=CLASS_NAMES,\n",
    "    yticklabels=CLASS_NAMES,\n",
    "    ax=axes[0],\n",
    "    cbar_kws={'label': 'Proportion'}\n",
    ")\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_title('Normalized Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Absolute confusion matrix\n",
    "sns.heatmap(\n",
    "    results.confusion_matrix, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=CLASS_NAMES,\n",
    "    yticklabels=CLASS_NAMES,\n",
    "    ax=axes[1],\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_title('Absolute Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'outputs' / 'confusion_matrix_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231152",
   "metadata": {},
   "source": [
    "## 7. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364df56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics DataFrame\n",
    "per_class_df = pd.DataFrame(results.per_class_metrics).T\n",
    "per_class_df['Class'] = per_class_df.index\n",
    "per_class_df = per_class_df[['Class', 'accuracy', 'precision', 'recall', 'support']]\n",
    "per_class_df.columns = ['Class', 'Accuracy', 'Precision', 'Recall', 'Support']\n",
    "\n",
    "# Add F1-score per class\n",
    "per_class_df['F1-Score'] = 2 * (per_class_df['Precision'] * per_class_df['Recall']) / \\\n",
    "                           (per_class_df['Precision'] + per_class_df['Recall'] + 1e-8)\n",
    "\n",
    "# Sort by support (most samples first)\n",
    "per_class_df = per_class_df.sort_values('Support', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã PER-CLASS PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "display(per_class_df.style.format({\n",
    "    'Accuracy': '{:.4f}',\n",
    "    'Precision': '{:.4f}',\n",
    "    'Recall': '{:.4f}',\n",
    "    'F1-Score': '{:.4f}',\n",
    "    'Support': '{:,.0f}'\n",
    "}).background_gradient(subset=['Accuracy', 'Precision', 'Recall', 'F1-Score'], cmap='RdYlGn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart of metrics per class\n",
    "x = np.arange(len(CLASS_NAMES))\n",
    "width = 0.2\n",
    "\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics_to_plot, colors)):\n",
    "    values = [per_class_df[per_class_df['Class'] == c][metric].values[0] \n",
    "              for c in CLASS_NAMES]\n",
    "    axes[0].bar(x + i*width, values, width, label=metric, color=color, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Per-Class Metrics', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(CLASS_NAMES, rotation=45, ha='right')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Class distribution (support)\n",
    "supports = [per_class_df[per_class_df['Class'] == c]['Support'].values[0] \n",
    "            for c in CLASS_NAMES]\n",
    "colors_pie = plt.cm.Set3(np.linspace(0, 1, len(CLASS_NAMES)))\n",
    "\n",
    "wedges, texts, autotexts = axes[1].pie(\n",
    "    supports, \n",
    "    labels=CLASS_NAMES, \n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors_pie,\n",
    "    explode=[0.02]*len(CLASS_NAMES)\n",
    ")\n",
    "axes[1].set_title('Class Distribution in Test Set', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d667d",
   "metadata": {},
   "source": [
    "## 8. ROC Curves and AUC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b278db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curves for each class\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Binarize labels for one-vs-rest\n",
    "y_true_bin = label_binarize(results.labels, classes=range(NUM_CLASSES))\n",
    "y_score = results.probabilities\n",
    "\n",
    "# Compute ROC curve and AUC for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    if y_true_bin[:, i].sum() > 0:  # Only if class exists in test set\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    else:\n",
    "        fpr[i], tpr[i], roc_auc[i] = [0, 1], [0, 1], 0.5\n",
    "\n",
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, NUM_CLASSES))\n",
    "\n",
    "for i, (class_name, color) in enumerate(zip(CLASS_NAMES, colors)):\n",
    "    ax.plot(\n",
    "        fpr[i], tpr[i],\n",
    "        color=color,\n",
    "        lw=2,\n",
    "        label=f'{class_name} (AUC = {roc_auc[i]:.3f})'\n",
    "    )\n",
    "\n",
    "# Plot diagonal (random classifier)\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.500)')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('Multi-Class ROC Curves (One-vs-Rest)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'outputs' / 'roc_curves_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print AUC summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AUC-ROC per Class\")\n",
    "print(\"=\"*50)\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  {class_name:15s}: {roc_auc[i]:.4f}\")\n",
    "print(f\"\\n  {'Macro Average':15s}: {np.mean(list(roc_auc.values())):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747ac49",
   "metadata": {},
   "source": [
    "## 9. Prediction Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "confidence = np.max(results.probabilities, axis=1)\n",
    "correct = results.predictions == results.labels\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Overall confidence distribution\n",
    "axes[0].hist(confidence, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(confidence.mean(), color='red', linestyle='--', lw=2, label=f'Mean: {confidence.mean():.3f}')\n",
    "axes[0].set_xlabel('Prediction Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Confidence: Correct vs Incorrect\n",
    "axes[1].hist(confidence[correct], bins=30, alpha=0.7, label=f'Correct (n={correct.sum():,})', color='green')\n",
    "axes[1].hist(confidence[~correct], bins=30, alpha=0.7, label=f'Incorrect (n={(~correct).sum():,})', color='red')\n",
    "axes[1].set_xlabel('Prediction Confidence', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Confidence by Correctness', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# Accuracy vs Confidence bins\n",
    "confidence_bins = np.linspace(0, 1, 11)\n",
    "bin_indices = np.digitize(confidence, confidence_bins)\n",
    "bin_accuracies = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(1, len(confidence_bins)):\n",
    "    mask = bin_indices == i\n",
    "    if mask.sum() > 0:\n",
    "        bin_accuracies.append(correct[mask].mean())\n",
    "        bin_counts.append(mask.sum())\n",
    "    else:\n",
    "        bin_accuracies.append(0)\n",
    "        bin_counts.append(0)\n",
    "\n",
    "bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "\n",
    "ax3 = axes[2]\n",
    "ax3.bar(bin_centers, bin_accuracies, width=0.08, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax3.plot([0, 1], [0, 1], 'r--', lw=2, label='Perfect Calibration')\n",
    "ax3.set_xlabel('Confidence', fontsize=12)\n",
    "ax3.set_ylabel('Accuracy', fontsize=12)\n",
    "ax3.set_title('Calibration: Confidence vs Accuracy', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlim(0, 1)\n",
    "ax3.set_ylim(0, 1.05)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confidence statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Confidence Statistics\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Mean Confidence:     {confidence.mean():.4f}\")\n",
    "print(f\"  Median Confidence:   {np.median(confidence):.4f}\")\n",
    "print(f\"  Std Confidence:      {confidence.std():.4f}\")\n",
    "print(f\"  Correct Mean Conf:   {confidence[correct].mean():.4f}\")\n",
    "print(f\"  Incorrect Mean Conf: {confidence[~correct].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9029802",
   "metadata": {},
   "source": [
    "## 10. Misclassification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2632627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common misclassifications\n",
    "misclass_pairs = []\n",
    "for i in range(len(results.labels)):\n",
    "    if results.predictions[i] != results.labels[i]:\n",
    "        true_class = CLASS_NAMES[results.labels[i]]\n",
    "        pred_class = CLASS_NAMES[results.predictions[i]]\n",
    "        conf = results.probabilities[i, results.predictions[i]]\n",
    "        misclass_pairs.append((true_class, pred_class, conf))\n",
    "\n",
    "# Count misclassification patterns\n",
    "from collections import Counter\n",
    "pattern_counts = Counter([(t, p) for t, p, _ in misclass_pairs])\n",
    "top_misclass = pattern_counts.most_common(10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç TOP 10 MISCLASSIFICATION PATTERNS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'True Class':<15} {'Predicted As':<15} {'Count':>10} {'%':>8}\")\n",
    "print(\"-\"*60)\n",
    "total_errors = len(misclass_pairs)\n",
    "for (true_cls, pred_cls), count in top_misclass:\n",
    "    pct = count / total_errors * 100 if total_errors > 0 else 0\n",
    "    print(f\"{true_cls:<15} {pred_cls:<15} {count:>10,} {pct:>7.1f}%\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Total Errors':<31} {total_errors:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e2238",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON\n",
    "export_results = {\n",
    "    \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"model_variant\": MODEL_VARIANT,\n",
    "    \"checkpoint_path\": str(CHECKPOINT_PATH),\n",
    "    \"num_samples\": len(results.labels),\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": float(results.accuracy),\n",
    "        \"balanced_accuracy\": float(results.balanced_accuracy),\n",
    "        \"precision_macro\": float(results.precision_macro),\n",
    "        \"recall_macro\": float(results.recall_macro),\n",
    "        \"f1_macro\": float(results.f1_macro),\n",
    "        \"f1_weighted\": float(results.f1_weighted),\n",
    "        \"auc_macro\": float(results.auc_macro) if results.auc_macro else None\n",
    "    },\n",
    "    \"per_class_metrics\": results.per_class_metrics,\n",
    "    \"per_class_auc\": {CLASS_NAMES[i]: float(roc_auc[i]) for i in range(NUM_CLASSES)},\n",
    "    \"confusion_matrix\": results.confusion_matrix.tolist(),\n",
    "    \"confidence_stats\": {\n",
    "        \"mean\": float(confidence.mean()),\n",
    "        \"std\": float(confidence.std()),\n",
    "        \"correct_mean\": float(confidence[correct].mean()),\n",
    "        \"incorrect_mean\": float(confidence[~correct].mean()) if (~correct).sum() > 0 else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_path = project_root / 'outputs' / f'evaluation_results_{DATASET_NAME}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(export_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Results exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf04a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive evaluation metrics for your trained DSCATNet model:\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| **Accuracy** | Standard overall accuracy | May be misleading with imbalanced data |\n",
    "| **Balanced Accuracy** | Mean recall across classes | Better for imbalanced datasets |\n",
    "| **F1 (macro)** | Unweighted mean F1 | Treats all classes equally |\n",
    "| **F1 (weighted)** | Weighted by class support | Accounts for class imbalance |\n",
    "| **AUC-ROC** | Multi-class one-vs-rest | Measures ranking ability |\n",
    "\n",
    "**Key insights:**\n",
    "- Check per-class metrics for underperforming classes (often minority classes)\n",
    "- Analyze confusion matrix for systematic misclassification patterns\n",
    "- Review confidence calibration for deployment decisions\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
